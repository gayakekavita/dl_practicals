{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lyUug1p8PfIM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zmuEPbThPfIR"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "NEVIKMeuPfIS"
   },
   "outputs": [],
   "source": [
    "sentences = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__CNxl2UPfIT"
   },
   "source": [
    "Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "guVroTiIPfIW"
   },
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n",
    "\n",
    "# remove 1 letter words\n",
    "sentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n",
    "\n",
    "# lower all characters\n",
    "sentences = sentences.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyL0meVEPfIX"
   },
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "N5XwBIU5PfIY"
   },
   "outputs": [],
   "source": [
    "words = sentences.split()\n",
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5iZfBd8wPfIZ"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 10\n",
    "context_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGsVKu5tPfIa"
   },
   "source": [
    "Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7XDAXNH3PfIa"
   },
   "outputs": [],
   "source": [
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98gc5ukRPfIb"
   },
   "source": [
    "Data bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ykFeOb8rPfIc",
    "outputId": "d3b44fc2-7095-4666-b956-9d98f9f781dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['we', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'computational'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "# data - [(context), target]\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrwWGpesPfIe"
   },
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "MfmVo4VmPfIe"
   },
   "outputs": [],
   "source": [
    "embeddings =  np.random.random_sample((vocab_size, embed_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Y2wtBkuPfIf"
   },
   "source": [
    "Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "k6-SH5GsPfIf"
   },
   "outputs": [],
   "source": [
    "def linear(m, theta):\n",
    "    w = theta\n",
    "    return m.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2mOfVX0PfIg"
   },
   "source": [
    "Log softmax + NLLloss = Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Ceiyio5GPfIh"
   },
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.log(e_x / e_x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "o65Di12EPfIh"
   },
   "outputs": [],
   "source": [
    "def NLLLoss(logs, targets):\n",
    "    out = logs[range(len(targets)), targets]\n",
    "    return -out.sum()/len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Uqu8IDATPfIi"
   },
   "outputs": [],
   "source": [
    "def log_softmax_crossentropy_with_logits(logits,target):\n",
    "\n",
    "    out = np.zeros_like(logits)\n",
    "    out[np.arange(len(logits)),target] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- out + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfhlI2kZPfIi"
   },
   "source": [
    "Forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "tJZlcnJhPfIj"
   },
   "outputs": [],
   "source": [
    "def forward(context_idxs, theta):\n",
    "    m = embeddings[context_idxs].reshape(1, -1)\n",
    "    n = linear(m, theta)\n",
    "    o = log_softmax(n)\n",
    "    \n",
    "    return m, n, o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQqB-gHePfIj"
   },
   "source": [
    "Backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "lAdsYZFbPfIk"
   },
   "outputs": [],
   "source": [
    "def backward(preds, theta, target_idxs):\n",
    "    m, n, o = preds\n",
    "    \n",
    "    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n",
    "    dw = m.T.dot(dlog)\n",
    "    \n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcsjEB6nPfIk"
   },
   "source": [
    "Optimize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "l29zeflgPfIk"
   },
   "outputs": [],
   "source": [
    "def optimize(theta, grad, lr=0.03):\n",
    "    theta -= grad * lr\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBRagYDmPfIl"
   },
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "d522PkF5PfIl"
   },
   "outputs": [],
   "source": [
    "theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "28SbjK1nPfIl"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for epoch in range(80):\n",
    "\n",
    "    losses =  []\n",
    "\n",
    "    for context, target in data:\n",
    "        context_idxs = np.array([word_to_ix[w] for w in context])\n",
    "        preds = forward(context_idxs, theta)\n",
    "\n",
    "        target_idxs = np.array([word_to_ix[target]])\n",
    "        loss = NLLLoss(preds[-1], target_idxs)\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        grad = backward(preds, theta, target_idxs)\n",
    "        theta = optimize(theta, grad, lr=0.03)\n",
    "        \n",
    "     \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sz8G-_xCPfIn"
   },
   "source": [
    "Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "G_9WRoMNPfIn"
   },
   "outputs": [],
   "source": [
    "def predict(words):\n",
    "    context_idxs = np.array([word_to_ix[w] for w in words])\n",
    "    preds = forward(context_idxs, theta)\n",
    "    word = ix_to_word[np.argmax(preds[-1])]\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "_k9zeF1HPfIo",
    "outputId": "58dd7c69-dfb9-42e1-b455-7a2687eb45b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'about'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (['we', 'are', 'to', 'study'], 'about')\n",
    "predict(['we', 'are', 'to', 'study'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2xFh09yPfIo"
   },
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "H5dFKGk9PfIp"
   },
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    wrong = 0\n",
    "\n",
    "    for context, target in data:\n",
    "        if(predict(context) != target):\n",
    "            wrong += 1\n",
    "            \n",
    "    return (1 - (wrong / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "xs7e0aEHPfIp",
    "outputId": "d186085f-c0fd-42bb-cdb5-3a69c3ad0e1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "2Y0O0lB7PfIq",
    "outputId": "f4873c5b-4d52-4636-fb12-2819e0a30c64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(['processes', 'manipulate', 'things', 'study'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ". Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n",
    "a. Data preparation\n",
    "b. Generate training data\n",
    "c. Train model\n",
    "d. Output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CBOW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
